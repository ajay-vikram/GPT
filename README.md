# Generative Pretrained Transformer (GPT)

This repo contains a naive PyTorch implementation of the GPT model.

## File Structure
```
├──bigram.py                                --> bigram language model
├──gpt.py                                   --> transformer decoder-only model
├──input.txt                                --> shakespeare training data
```

## Transformer Architecture
![GitHub Logo](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)
